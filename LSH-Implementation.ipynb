{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAJo-Joxjwv_"
   },
   "source": [
    "# Locality Sensitive Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure as fg\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sOmD2gPPjwv_"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import binascii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_csv('preproc_art_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_c_p = dict()\n",
    "encoded_p_c = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodes a type in a sequencial int\n",
    "def encode_property(property_name, plaintext):\n",
    "  global encoded_c_p, encoded_p_c\n",
    "  code = 0\n",
    "  plaintext_str = str(plaintext)\n",
    "  try:\n",
    "    tmp = encoded_p_c[property_name] #Check if property is in dict\n",
    "    try:\n",
    "      code = tmp[plaintext_str] #Check if category is in dict\n",
    "    except:\n",
    "      encoded_p_c[property_name][plaintext_str] = binascii.crc32(bytes(plaintext,'utf-8'))\n",
    "      encoded_c_p[property_name][encoded_p_c[property_name][plaintext_str]] = plaintext_str\n",
    "  except:\n",
    "    encoded_c_p[property_name] = dict()\n",
    "    encoded_p_c[property_name] = dict()\n",
    "    encoded_p_c[property_name][plaintext_str] = binascii.crc32(bytes(plaintext,'utf-8'))\n",
    "    encoded_c_p[property_name][encoded_p_c[property_name][plaintext_str]] = plaintext_str\n",
    "  code = encoded_p_c[property_name][plaintext_str]\n",
    "  return code\n",
    "\n",
    "def decode_property(property_name, code):\n",
    "  global encoded_c_p\n",
    "  tmp = encoded_c_p[property_name] #Check if property is in dict\n",
    "  plaintext = tmp[code] #Check if category is in dict\n",
    "  return plaintext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shingles=list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_docs = list(articles.article.apply(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function creates n-gram\n",
    "def crete_grams(doc, n = 3):\n",
    "    grams = []\n",
    "    for tokens_len in range(len(doc.split())):\n",
    "        if tokens_len + n > len(doc.split()):\n",
    "            break\n",
    "        grams.append(' '.join(doc.split()[tokens_len : tokens_len+n]))\n",
    "    return grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create the shingles of all the text\n",
    "for doc in tqdm(list_docs):\n",
    "    shingles.extend(crete_grams(doc, n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the shingles are encoded in  32 bit integer\n",
    "for gram in tqdm(shingles):\n",
    "    encode_property('sh',gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function given a text will return the codes corresponding to each of its shingles\n",
    "def convert_text_in_shingles(text, n_grams = 3):\n",
    "    shingles = crete_grams(text, n_grams)\n",
    "    hashed_text = []\n",
    "    for shingle in shingles:\n",
    "        hashed_text.append(encode_property('sh', shingle)) # Encode, if the word is already present returns the corresponding words\n",
    "    return hashed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles['sh'] = articles.article.progress_apply(lambda x: convert_text_in_shingles(str(x), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SignMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maximum used shingle hash\n",
    "max_sh_ID = 2**32-1\n",
    "print(max_sh_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used to calculate a, b\n",
    "def rand_k_coeff(k):\n",
    "    ret_list = []\n",
    "    for x in range(k):\n",
    "        found = True\n",
    "        while(found):\n",
    "            ran_int = random.randint(0, max_sh_ID)\n",
    "            if ran_int not in ret_list:\n",
    "                ret_list.append(ran_int)\n",
    "                found = False\n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://compoasso.free.fr/primelistweb/page/prime/liste_online_en.php\n",
    "next_prime = 4294967311"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ret (ax+b)%next_prime, next_prime:max shingle index, x: shingle id a,b:rand num\n",
    "def get_hash_funcs(shingles, n_esec, a, b):\n",
    "    global next_prime\n",
    "    min_hash = next_prime +1\n",
    "    for shingle in shingles:\n",
    "        hash_code = ((a[n_esec] * shingle + b[n_esec]) % next_prime)\n",
    "        if hash_code < min_hash:\n",
    "            min_hash = hash_code\n",
    "    return int(min_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate signature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sign_matr(num_hash_fun, docs_df):\n",
    "    a = rand_k_coeff(num_hash_fun)\n",
    "    b = rand_k_coeff(num_hash_fun)\n",
    "    signature_matr = np.zeros((num_hash_fun, len(list_docs)), int)\n",
    "    for y in tqdm(range(num_hash_fun)):\n",
    "        signature_matr[y, :] = (docs_df['sh'].apply(lambda x: get_hash_funcs(x, y, a, b)))\n",
    "    return signature_matr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert an element in the correct bucket\n",
    "def check_bucket(bucket_hash, hash, id):\n",
    "    try: # If there is already this hash in the bucket we will add the id\n",
    "        similar_list = bucket_hash[hash]\n",
    "        similar_list.append(id)\n",
    "        bucket_hash[hash] = similar_list\n",
    "    except:# Otherwise we create a cell in the bucket for this hash\n",
    "        bucket_hash[hash] = [id]\n",
    "    return bucket_hash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "# Construct the buckets\n",
    "def get_buckets_LSH(sign_matr, band, rows):\n",
    "    buckets_id = []\n",
    "    num_hash_fun = sign_matr.shape[0]\n",
    "    num_docs = sign_matr.shape[1]\n",
    "    tmp_sign = sign_matr[:(band*rows),:].copy()\n",
    "    if band*rows > num_hash_fun:\n",
    "        return 0\n",
    "    for band in range(band):\n",
    "        actual_band = tmp_sign[band*rows:(band+1)*rows,:] #get the band hashes\n",
    "        bucket_hash = dict()\n",
    "        for doc_hash_band in range(num_docs): #Iterate on the hashes\n",
    "            final_hash = ''.join(list(map(str,list(actual_band[:, doc_hash_band]))))\n",
    "            bucket_hash = check_bucket(bucket_hash, final_hash, doc_hash_band)\n",
    "        bucket_hash = {k: v for k, v in bucket_hash.items() if len(v) >= 2}\n",
    "        buckets_id.append(bucket_hash)\n",
    "    return buckets_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, groupby\n",
    "# Resturns from the buckets the list of candidate pairs\n",
    "def get_candidate_pairs(buckets):\n",
    "    pairs = []\n",
    "    if buckets == 0:\n",
    "        return pairs\n",
    "    for bucket in buckets:\n",
    "        pairs.extend(bucket.values())\n",
    "    pairs.sort()\n",
    "    pairs = list(k for k,_ in groupby(pairs))\n",
    "    pairs_2 = list(filter(lambda x: len(x) <3, pairs))\n",
    "    pairs_3 = list(filter(lambda x: len(x) >2, pairs))\n",
    "    tmp_pairs = []\n",
    "    for x in pairs_3:\n",
    "        tmp_pairs.extend(list(map(list, list(combinations(x, 2)))))\n",
    "    pairs_2.extend(tmp_pairs)\n",
    "    pairs_2.sort()\n",
    "    pairs = list(k for k,_ in groupby(pairs_2))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_truth = np.load('g_truth.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(g_truth, candidates, similarity): # calculates tp, tn, fp, fn\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    truth_indexes = np.argwhere(g_truth >= similarity)\n",
    "    false_indexes = len(np.argwhere(g_truth < similarity)) - int(((g_truth.shape[0]**2)-g_truth.shape[0])/2) - g_truth.shape[0] \n",
    "    for index in (truth_indexes):\n",
    "        if len(candidates) == 0:\n",
    "            return [0,0,0]\n",
    "        ret_list = list(candidates[(candidates[0] == index[0])][1])\n",
    "        ret_list.extend(list(candidates[(candidates[1] == index[0])][0]))\n",
    "        if index[1] in ret_list:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "    fp = len(candidates) - tp\n",
    "    tn = false_indexes - fn\n",
    "    return [tp, fp, fn, tn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = lambda tp, fp: tp/(tp+fp)\n",
    "sensitivity = lambda tp, fn: tp/(tp + fn)\n",
    "specificity = lambda tn, fp: tn/(tn+fp)\n",
    "falseposrate = lambda fp, tn: fp/(fp+tn)\n",
    "falsenegrate = lambda fn, tp: fn/(fn+tp)\n",
    "accuracy = lambda tn, tp, fn, fp: (tp+tn)/(tp+tn+fp+fn)\n",
    "f1 = lambda tp, fn, fp: (2*tp)/((2*tp)+fp+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_score(tp, fp, fn, tn):\n",
    "    global precision, specificity, sensitivity, accuracy, falseposrate, f1, falsenegrate\n",
    "    print(\"Precision:\\t \" + str(round(precision(tp, fp)*100,2)))\n",
    "    print(\"F1:\\t\\t \" + str(round(f1(tp, fn, fp)*100,2)))\n",
    "    print(\"Accuracy:\\t \" + str(round(accuracy(tn, tp, fn, fp)*100,2)))\n",
    "    print(\"FPR:\\t\\t \" + str(round(falseposrate(fp, tn)*100,2)))\n",
    "    print(\"FNR:\\t\\t \" + str(round(falsenegrate(fn, tp)*100,2)))\n",
    "    print(\"Specificity:\\t \" + str(round(specificity(tn, fp)*100,2)))\n",
    "    print(\"Sensitivity:\\t \" + str(round(sensitivity(tp, fn)*100,2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_res(res, w_prec, w_spec, w_sens): # Use a weighted mean to select the best result\n",
    "    res['mean'] = res.iloc[:,3:6].apply(lambda x: ((w_prec*x[0])+(w_spec*x[1])+(w_sens*x[2]))/(w_prec+w_spec+w_sens), axis = 1)\n",
    "    res = res.sort_values(by=['mean'], ascending=False).reset_index(drop=True)\n",
    "    return res.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_matrix = create_sign_matr(1000, articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sim = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "test_R = range(3,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform tests on the combination of different number of rows, returns the scores prec, sens, spec\n",
    "def grid_search(test_R, p1, tolerance, signature_matrix, g_truth, similarity = 0.8):\n",
    "    res_matr = np.zeros((1, 6))\n",
    "    count = 0\n",
    "    for testing_R in tqdm(test_R):\n",
    "        testing_L = round(math.log(1-p1, (1-((similarity-tolerance)**testing_R)))) #calculates R\n",
    "        if(testing_L*testing_R > signature_matrix.shape[0]): continue # Verify the selected number of bands is coherent\n",
    "        tmp_signature = signature_matrix[:(testing_L*testing_R),:].copy()\n",
    "        # Perform LSH\n",
    "        buckets = get_buckets_LSH(tmp_signature, rows = testing_R, band=testing_L)\n",
    "        pairs = get_candidate_pairs(buckets)\n",
    "        if(len(pairs) < 1): continue\n",
    "        pairs = pd.DataFrame(pairs)\n",
    "        tp, fp, fn, tn = evaluate(g_truth, pairs, similarity)\n",
    "        p = round(precision(tp, fp)*100,2)\n",
    "        sen = round(sensitivity(tp, fn)*100,2)\n",
    "        spe = round(specificity(tn, fp)*100,3)\n",
    "        res_matr = np.vstack([res_matr, [(testing_L*testing_R), testing_R, testing_L, p, spe, sen]])\n",
    "    count +=1\n",
    "    res_matr = res_matr[1:,:]\n",
    "    return pd.DataFrame(res_matr, columns =['Num_Hash', 'Rows', 'Bands', 'Prec', 'Spec', 'Sens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 0.15\n",
    "analysis_dict = {}\n",
    "for x in test_sim:\n",
    "    res = grid_search(test_R, 0.95, tol, signature_matrix, g_truth, similarity=x)\n",
    "    analysis_dict[x*100] = get_best_res(res, 3,1,2).iloc[0].tolist()\n",
    "print(analysis_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg(figsize=(12, 8), dpi=80)\n",
    "x = list(map(int, analysis_dict.keys()))\n",
    "y_sen = [d[5] for d in analysis_dict.values()]\n",
    "y_spec = [d[4] for d in analysis_dict.values()]\n",
    "y_prec = [d[3] for d in analysis_dict.values()]\n",
    "plt_hash = [d[0] for d in analysis_dict.values()]\n",
    "plt_rows = [d[1] for d in analysis_dict.values()]\n",
    "plt_bands = [d[2] for d in analysis_dict.values()]\n",
    "width = 0.80\n",
    "\n",
    "# plot data in grouped manner of bar type\n",
    "for ind in range(len(x)):\n",
    "    plt.bar(x[ind]-0.9, y_prec[ind], width, color = 'blue')\n",
    "    plt.bar(x[ind]+0.9, y_spec[ind], width, color = 'green')\n",
    "    plt.bar(x[ind], y_sen[ind], width, color = 'orange')\n",
    "plt.ylabel('%')\n",
    "for i in range(len(x)):\n",
    "    plt.text(x[i]-1.2, y_prec[i]+1.5, str(y_prec[i]) + '%', color='blue', fontweight='bold', rotation = 90)\n",
    "    plt.text(x[i]+.7, y_spec[i]+1.5, str(y_spec[i]) + '%', color='green', fontweight='bold', rotation = 90)\n",
    "    plt.text(x[i]-0.2, y_sen[i]+1.5, str(y_sen[i]) + '%', color='orange', fontweight='bold', rotation = 90)\n",
    "\n",
    "    plt.text(x[i]+1.5, 50+13, \"N° Hash:\\n \"+str(plt_hash[i]), color='black', fontweight='bold')\n",
    "    plt.text(x[i]+1.5, 50+7, \"N° Bands:\\n \" + str(plt_bands[i]), color='black', fontweight='bold')\n",
    "    plt.text(x[i]+1.5, 50+1, \"N° Rows:\\n \" + str(plt_rows[i]), color='black', fontweight='bold')\n",
    "    p1 = str( 1-(1-((x[i]/100 - tol)**plt_rows[i]))**plt_bands[i] )[:4]\n",
    "    p2 = str(1-(1-((x[i]/100)**plt_rows[i]))**plt_bands[i])[:4]\n",
    "    s1 = str(x[i] - (tol*100))\n",
    "    plt.text(x[i]+1.5, 50-5, \"(\" + s1 + \"%, \" + str(x[i]) + \"%,\\n\" + p1 + \", \" + p2+\")\", color='black', fontweight='bold')\n",
    "plt.legend([\"Prec\", \"Spec\", \"Sens\"], loc = 'lower left')\n",
    "plt.xlabel('%Similarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert in final_sum for each similarity range (e.g., 0.0 - 0.1, ...) the count of pairs in that range\n",
    "final_sum = []\n",
    "step = 0.1\n",
    "sim_range = np.arange(0.0, 1.0, step)\n",
    "for sim_step in sim_range:\n",
    "    next_step = sim_step + step\n",
    "    if sim_step == 0.0:\n",
    "        elems = np.argwhere((g_truth >= sim_step) & (g_truth < next_step))\n",
    "        final_sum.append(len(elems) - 499500)\n",
    "        continue\n",
    "    elems = np.argwhere((g_truth >= sim_step) & (g_truth < next_step))\n",
    "    final_sum.append(len(elems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_down(n, decimals=0):\n",
    "    multiplier = 10 ** decimals\n",
    "    return math.floor(n * multiplier) / multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "fig = plt.figure(figsize=(10, 15), dpi=80)\n",
    "gs = fig.add_gridspec(3, hspace=0.1)\n",
    "axs = gs.subplots(sharex=True, sharey=True)\n",
    "\n",
    "x_bar = [0,10,20,30,40,50,60,70,80,90]\n",
    "y_bar = final_sum\n",
    "\n",
    "#p1 (65, 80, 95, 99)\n",
    "x = np.arange(0, 100, 0.01)\n",
    "y_outputs = (list(map(round_down,(1-(1-(x/100)**8)**93)*100)))\n",
    "\n",
    "ax02 = axs[0].twinx()\n",
    "\n",
    "axs[0].bar(x_bar, y_bar, color ='blue', width = 9.5, align='edge', log=10)\n",
    "\n",
    "for v in range(len(final_sum)):\n",
    "    axs[0].text(x_bar[v]+3.5, np.log10(y_bar[v])*9, str(final_sum[v]), color='orange', fontweight='bold', rotation = 90, size = 20)\n",
    "\n",
    "ax02.axvline(x=65, color = 'blue', linestyle = ':')\n",
    "ax02.axvline(x=80, color = 'blue', linestyle = ':')\n",
    "ax02.axhline(y=95, color = 'blue', linestyle = ':')\n",
    "ax02.axhline(y=99, color = 'blue', linestyle = ':')\n",
    "\n",
    "\n",
    "\n",
    "ax02.plot(x, y_outputs)\n",
    "#ax02.set_ylim([0, ymax])\n",
    "ax02.text(67,50, 'Tolerance', color='blue', fontweight='bold')\n",
    "ax02.text(53,30, 'FalsePositive', color='blue', fontweight='bold')\n",
    "ax02.text(82,101, 'FalseNegative', color='blue', fontweight='bold')\n",
    "ax02.set_title('(65%,80%,95%,99%)')\n",
    "\n",
    "#p2 (75, 80, 95, 99)\n",
    "x = np.arange(0, 100, 0.01)\n",
    "y_outputs = (list(map(round_down,(1-(1-(x/100)**10)**52)*100)))\n",
    "\n",
    "ax12 = axs[1].twinx()\n",
    "\n",
    "axs[1].bar(x_bar, y_bar, color ='blue', width = 9.5, align='edge', log=10)\n",
    "\n",
    "for v in range(len(final_sum)):\n",
    "    axs[1].text(x_bar[v]+3.5, np.log10(y_bar[v])*9, str(final_sum[v]), color='orange', fontweight='bold', rotation = 90, size = 20)\n",
    "\n",
    "ax12.axvline(x=75, color = 'blue', linestyle = ':')\n",
    "ax12.axvline(x=80, color = 'blue', linestyle = ':')\n",
    "ax12.axhline(y=95, color = 'blue', linestyle = ':')\n",
    "ax12.axhline(y=99, color = 'blue', linestyle = ':')\n",
    "\n",
    "ax12.plot(x, y_outputs)\n",
    "ax12.text(70,50, 'Tolerance', color='blue', fontweight='bold')\n",
    "ax12.text(60,30, 'FalsePositive', color='blue', fontweight='bold')\n",
    "ax12.text(82,101, 'FalseNegative', color='blue', fontweight='bold')\n",
    "ax12.set_title('(75%,80%,95%,99%)')\n",
    "\n",
    "#p3 (50, 80, 85, 99)\n",
    "x = np.arange(0, 100, 0.01)\n",
    "y_outputs = (list(map(round_down,(1-(1-(x/100)**6)**120)*100)))\n",
    "\n",
    "ax22 = axs[2].twinx()\n",
    "\n",
    "axs[2].bar(x_bar, y_bar, color ='blue', width = 9.5, align='edge', log=10)\n",
    "\n",
    "for v in range(len(final_sum)):\n",
    "    axs[2].text(x_bar[v]+3.5, np.log10(y_bar[v])*9, str(final_sum[v]), color='orange', fontweight='bold', rotation = 90, size = 20)\n",
    "\n",
    "ax22.axvline(x=50, color = 'blue', linestyle = ':')\n",
    "ax22.axvline(x=80, color = 'blue', linestyle = ':')\n",
    "ax22.axhline(y=85, color = 'blue', linestyle = ':')\n",
    "ax22.axhline(y=99, color = 'blue', linestyle = ':')\n",
    "\n",
    "\n",
    "ax22.plot(x, y_outputs)\n",
    "ax22.text(60,50, 'Tolerance', color='blue', fontweight='bold')\n",
    "ax22.text(35,35, 'FalsePositive', color='blue', fontweight='bold')\n",
    "ax22.text(82,101, 'FalseNegative', color='blue', fontweight='bold')\n",
    "ax22.set_title('(50%,80%,85%,99%)')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Large Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_large = pd.read_csv(\"preproc_art_large.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_c_p = dict()\n",
    "encoded_p_c = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shingles=list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_docs = list(articles_large.article.apply(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in tqdm(list_docs):\n",
    "    shingles.extend(crete_grams(doc, n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gram in tqdm(shingles):\n",
    "    encode_property('sh',gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_large['sh'] = articles_large.article.progress_apply(lambda x: convert_text_in_shingles(str(x), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_sign_matrix = create_sign_matr(744, articles_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets = get_buckets_LSH(large_sign_matrix, rows = 8, band=93)\n",
    "pairs = get_candidate_pairs(buckets)\n",
    "pairs = pd.DataFrame(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pairs.rename(columns={0: 'doc_id1', 1: 'doc_id2'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs.to_csv('result.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19649b6d477f04954267d7dfcc1e3219afd53992c8847ec6a09d5cd5145b7914"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
